# Graduation-Project

This project is based on the audio/vedio emotion competition.

## Models

### TEXT

|       Model       | Accuracy | Precision | Recall | F1 Score |
| :---------------: | :------: | :-------: | :----: | :------: |
| Embedding Average |          |           |        |          |
|      biLSTM       |          |           |        |          |
| biLSTM+attention  |          |           |        |          |
|        HAN        |          |           |        |          |

### VIDEO

- I'm going to use OpenFace to extract face landmarks to train a model which can capture people's expression

- [Real-world Affective Faces Database](http://www.whdeng.cn/RAF/model1.html)

- [Expression in-the-Wild (ExpW) Dataset](http://mmlab.ie.cuhk.edu.hk/projects/socialrelation/index.html)

  

### AUDIO



## Reference

* Yang, Zichao, et al. "Hierarchical attention networks for document classification." *Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies*. 2016.

* **OpenFace 2.0: Facial Behavior Analysis Toolkit** Tadas Baltru≈°aitis, Amir Zadeh, Yao Chong Lim, and Louis-Philippe Morency, *IEEE International Conference on Automatic Face and Gesture Recognition*, 2018

  



