# Graduation-Project

This project is based on the audio/vedio emotion competition.

## Models

### TEXT

|       Model       | Accuracy | Precision | Recall | F1 Score |
| :---------------: | :------: | :-------: | :----: | :------: |
| Embedding Average |          |           |        |          |
|      biLSTM       |          |           |        |          |
| biLSTM+attention  |          |           |        |          |
|        HAN        |          |           |        |          |

### VIDEO

- [Here](https://github.com/FengBotao119/Graduation-Project/tree/master/face_model)

### AUDIO

- 

## Reference

* Yang, Zichao, et al. "Hierarchical attention networks for document classification." *Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies*. 2016.

* **OpenFace 2.0: Facial Behavior Analysis Toolkit** Tadas Baltru≈°aitis, Amir Zadeh, Yao Chong Lim, and Louis-Philippe Morency, *IEEE International Conference on Automatic Face and Gesture Recognition*, 2018

* Fabian Benitez-Quiroz C, Srinivasan R, Martinez A M. Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 5562-5570.



